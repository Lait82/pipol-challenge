# ðŸ“° Web Scraper con IA + BigQuery (Google Cloud Run Job)

Este proyecto es un challenge que scrapea artÃ­culos desde [yogonet.com](https://www.yogonet.com/international), extrae contenido relevante (usando IA o scraping clÃ¡sico), analiza mÃ©tricas de texto y las sube a BigQuery.


## ðŸš€ Features

- âœ… Scraping tradicional con Selenium
- ðŸ¤– ExtracciÃ³n con HuggingFace opcional (AI Based Scraping)
- ðŸ“Š MÃ©tricas: frecuencia de palabras, caracteres y palabras capitalizadas
- â˜ï¸ Upload automÃ¡tico a BigQuery
- ðŸ³ Dockerized
- ðŸ” Deploy automatizado como Google Cloud Run Job


## ðŸ“ Estructura

```
project/
â”‚
â”œâ”€â”€ main.py                  # Entrada principal del proceso
â”œâ”€â”€ Dockerfile               # Define el contenedor
â”œâ”€â”€ deploy.sh                # Script de deploy a Cloud Run
â”œâ”€â”€ .env                     # ConfiguraciÃ³n local y de producciÃ³n
â”‚
â”œâ”€â”€ scraping/                # LÃ³gica de scraping (con IA o no)
â”œâ”€â”€ data_processing/         # ExtracciÃ³n de mÃ©tricas
â”œâ”€â”€ clients/                 # Cliente de BigQuery
â””â”€â”€ loaders/                 # Carga de DataFrames a BigQuery
```


## ðŸ” .env de ejemplo

```env
GOOGLE_APPLICATION_CREDENTIALS=credentials.json
GOOGLE_PROJECT_ID=project_name
BIGQUERY_DATASET=news_data
HF_API_KEY=hf_xxx...

REGION=us-central1
REPO_NAME=scrapers
IMAGE_NAME=web-scraper
SERVICE_NAME=web-scraper
TAG=latest
AI_BASED_SCRAPING=false
```

> â˜ï¸ Este archivo se usa tanto localmente como para configurar las variables del entorno en Cloud Run.


## ðŸ§ª Ejecutar localmente

```bash
# Con scraping clÃ¡sico
python main.py --local

# Con scraping IA + HuggingFace
python main.py --local --ai-based-scraping
```

> El flag `--local` activa el uso de `.env` y los chequeos locales de dependencias.

## ðŸš€ Deploy a Cloud Run Job

```bash
./deploy.sh
```

Este script hace todo:
1. Carga las variables desde `.env`
2. Buildea la imagen Docker
3. La sube a Artifact Registry
4. Crea o actualiza el Job de Cloud Run
5. Ejecuta el Job automÃ¡ticamente


## ðŸªµ Ver logs manualmente

```bash
gcloud logging read "resource.type=cloud_run_job AND resource.labels.job_name=web-scraper" \
  --project=tu-proyecto-id \
  --limit=50 \
  --format="value(textPayload)"
```

## ðŸ“Œ Requisitos

- Tener configurado Google Cloud CLI (`gcloud`)
- Proyecto con billing activo y servicios habilitados:
  - Artifact Registry
  - Cloud Run
  - BigQuery
- Roles necesarios:
  - `Cloud Run Admin`
  - `Artifact Registry Writer`
  - `BigQuery Data Editor`



# ðŸ§  Tips
- Para producciÃ³n si querÃ©s usar el el scraping basado en IA usÃ¡ `AI_BASED_SCRAPING=true` en `.env`



## ðŸ“¬ Contacto

Cualquier bug, mejora o idea, Â¡mandÃ¡ PR o abrÃ­ un issue!